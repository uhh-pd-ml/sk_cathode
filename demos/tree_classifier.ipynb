{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak Supervision With Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebook `demos/weak_supervision.ipynb` we have seen that a neural network (NN) classifier trained to distinguish a signal+background sample from a background-only one will also learn to distinguish signal from background directly. We call this (unrealistic) case where we have a perfect background-only dataset an *idealized anomaly detector* (IAD), as it is an idealization of anomaly detection approaches, such as [CWoLa Hunting](https://arxiv.org/abs/1902.02634) and [CATHODE](https://arxiv.org/abs/2109.00546) where we obtain this background sample in a data-driven way. I.e. any challenges that we face with an IAD will very likely apply to weakly supervised anomaly detection methods in general.\n",
    "\n",
    "We will explore one such challenge in this notebook: the **sensitivity loss due to uninformative features**. In order to be sensitive to a broad range of anomalies (thus being maximally model agnostic), we would ideally add as many input features to the classifier as possible. For a specific type of signal, most of these features would then be uninformative. Unfortunately, it turns out in practice that this heavily decreases the sensitivity to that signal. The presence of a small signal within an overwhelming background is more and more washed out in more and more noisy dimensions, making it much harder to detect for the NN. This challenge was discussed in more depth in [this paper](https://arxiv.org/abs/2309.13111), where a substantial improvement was achieved by **replacing the NN classifier by a boosted decision tree (BDT) classifier**. The very different inductive bias of tree-based classifiers makes them more resilient to noisy features.\n",
    "\n",
    "This notebook builds on `demos/weak_supervision.ipynb`, using the same dataset and the same NN classifier. We will then train a [Histogram-based Gradient Boosting Classification Tree](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html) as an alternative to confirm it has similar performance as the NN when all features contain relevant information on the specific kind of signal that is present in the data. Then we will add ten uninformative features, simply by drawing random numbers from a normal distribution, and see how the two classifier models react differently. [The tree-based weak supervision paper](https://arxiv.org/abs/2309.13111) also discussed that the BDTs become even more stable at high amounts of noise if we *ensemble* them: train multiple BDTs with different train/validation splits and average their predictions. Thus, we will end the notebook with an implementation of such an emsemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from os.path import exists, join, dirname, realpath\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# adding parent directory to path\n",
    "parent_dir = dirname(realpath(globals()[\"_dh\"][0]))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from sk_cathode.classifier_models.boosted_decision_tree import HGBClassifier\n",
    "from sk_cathode.classifier_models.neural_network_classifier import NeuralNetworkClassifier\n",
    "from sk_cathode.utils.ensembling_utils import EnsembleModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :sunglasses:\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same input data as in `demos/weak_supervision.ipynb` are used here. We make use of the same separation into train/validation/test data. However, we don't make use of the extra train/validation signal, as we won't train a supervised classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./input_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation (download and high-level preprocessing)\n",
    "if not exists(join(data_path, \"innerdata_test.npy\")):\n",
    "    process = subprocess.run(f\"{sys.executable} {join(parent_dir, 'demos', 'utils', 'data_preparation.py')} --outdir {data_path}\", shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "innerdata_train = np.load(join(data_path, \"innerdata_train.npy\"))\n",
    "innerdata_val = np.load(join(data_path, \"innerdata_val.npy\"))\n",
    "innerdata_test = np.load(join(data_path, \"innerdata_test.npy\"))\n",
    "innerdata_extrabkg_train = np.load(join(data_path, \"innerdata_extrabkg_train.npy\"))\n",
    "innerdata_extrabkg_val = np.load(join(data_path, \"innerdata_extrabkg_val.npy\"))\n",
    "innerdata_extrabkg_test = np.load(join(data_path, \"innerdata_extrabkg_test.npy\"))\n",
    "innerdata_extrasig = np.load(join(data_path, \"innerdata_extrasig.npy\"))\n",
    "\n",
    "# Enriching the test set with extra signal.\n",
    "# We could use all, but this way it's consistent with previous notebooks.\n",
    "innerdata_extrasig_test = innerdata_extrasig[:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in `demos/weak_supervision.ipynb`, we train a classifier to distinguish between \"data\" and a pure background. We first start by again using a neural network. We even use the same model path, so we can recycle the previous one if it was trained in the other notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning label 1 to \"data\"\n",
    "clsf_train_data = innerdata_train.copy()\n",
    "clsf_train_data[:, -1] = np.ones_like(clsf_train_data[:, -1])\n",
    "clsf_val_data = innerdata_val.copy()\n",
    "clsf_val_data[:, -1] = np.ones_like(clsf_val_data[:, -1])\n",
    "\n",
    "# and label 0 to background\n",
    "clsf_train_bkg = innerdata_extrabkg_train.copy()\n",
    "clsf_train_bkg[:, -1] = np.zeros_like(clsf_train_bkg[:, -1])\n",
    "clsf_val_bkg = innerdata_extrabkg_val.copy()\n",
    "clsf_val_bkg[:, -1] = np.zeros_like(clsf_val_bkg[:, -1])\n",
    "\n",
    "# mixing together and shuffling\n",
    "clsf_train_set = np.vstack([clsf_train_data, clsf_train_bkg])\n",
    "clsf_val_set = np.vstack([clsf_val_data, clsf_val_bkg])\n",
    "clsf_train_set = shuffle(clsf_train_set, random_state=42)\n",
    "clsf_val_set = shuffle(clsf_val_set, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either train new NN classifier to distinguish between \"data\" and background\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(clsf_train_set[:, 1:-1])\n",
    "\n",
    "X_train = scaler.transform(clsf_train_set[:, 1:-1])\n",
    "y_train = clsf_train_set[:, -1]\n",
    "X_val = scaler.transform(clsf_val_set[:, 1:-1])\n",
    "y_val = clsf_val_set[:, -1]\n",
    "\n",
    "nn_classifier_savedir = \"./trained_classifiers_idealized-ad_0/\"\n",
    "# Let's protect ourselves from accidentally overwriting a trained model.\n",
    "if not exists(join(nn_classifier_savedir, \"CLSF_models\")):\n",
    "    nn_classifier_model = NeuralNetworkClassifier(save_path=nn_classifier_savedir,\n",
    "                                                  n_inputs=X_train.shape[1],\n",
    "                                                  early_stopping=True, epochs=None,\n",
    "                                                  verbose=True)\n",
    "    nn_classifier_model.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # merge scaler and classifier into a single pipeline\n",
    "    nn_full_model = make_pipeline(scaler, nn_classifier_model)\n",
    "else:\n",
    "    print(f\"The model exists already in {nn_classifier_savedir}. Remove first if you want to overwrite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or alternatively load existing classifer model\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(clsf_train_set[:, 1:-1])\n",
    "\n",
    "nn_classifier_savedir = \"./trained_classifiers_idealized-ad_0/\"\n",
    "nn_classifier_model = NeuralNetworkClassifier(save_path=nn_classifier_savedir,\n",
    "                                              n_inputs=clsf_train_set[:, 1:-1].shape[1],\n",
    "                                              load=True)\n",
    "nn_full_model = make_pipeline(scaler, nn_classifier_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we use significance improvement characteristic (SIC) curves to evalaute the sensitivity to this specific test signal in the LHCO data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's evaluate the signal extraction performance on the same test set\n",
    "\n",
    "clsf_test_set = np.vstack([innerdata_test,\n",
    "                           innerdata_extrabkg_test,\n",
    "                           innerdata_extrasig_test])\n",
    "\n",
    "X_test = clsf_test_set[:, 1:-1]\n",
    "y_test = clsf_test_set[:, -1]\n",
    "\n",
    "nn_preds_test = nn_full_model.predict(X_test).flatten()\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    nn_fpr, nn_tpr, _ = roc_curve(y_test, nn_preds_test)\n",
    "    nn_bkg_rej = 1 / nn_fpr\n",
    "    nn_sic = nn_tpr / np.sqrt(nn_fpr)\n",
    "\n",
    "    random_tpr = np.linspace(0, 1, 300)\n",
    "    random_bkg_rej = 1 / random_tpr\n",
    "    random_sic = random_tpr / np.sqrt(random_tpr)\n",
    "\n",
    "# SIC curve\n",
    "plt.plot(nn_tpr, nn_sic, label=\"idealized AD, NN\")\n",
    "plt.plot(random_tpr, random_sic, \"w:\", label=\"random\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we reproduced the neural network benchmark on the default four high-level input features of the LHCO data, let's train a Histogram-based Gradient Boosting Classification Tree on the exact same input data. Our implementation here wraps the scikit-learn implementation in a way to have more control over the validation loss and add the same model saving/loading functionality as we have in the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either train new HistGradientBoosting classifier to distinguish between \"data\" and background\n",
    "\n",
    "# note that the scaler here is the same as we used for the NN\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(clsf_train_set[:, 1:-1])\n",
    "\n",
    "X_train = scaler.transform(clsf_train_set[:, 1:-1])\n",
    "y_train = clsf_train_set[:, -1]\n",
    "X_val = scaler.transform(clsf_val_set[:, 1:-1])\n",
    "y_val = clsf_val_set[:, -1]\n",
    "\n",
    "bdt_classifier_savedir = \"./trained_classifiers_tree_idealized-ad_0/\"\n",
    "# Let's protect ourselves from accidentally overwriting a trained model.\n",
    "if not exists(join(bdt_classifier_savedir, \"CLSF_models\")):\n",
    "    bdt_classifier_model = HGBClassifier(save_path=bdt_classifier_savedir,\n",
    "                                         early_stopping=True, max_iters=None,\n",
    "                                         verbose=True)\n",
    "    bdt_classifier_model.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # merge scaler and classifier into a single pipeline\n",
    "    bdt_full_model = make_pipeline(scaler, bdt_classifier_model)\n",
    "else:\n",
    "    print(f\"The model exists already in {bdt_classifier_savedir}. Remove first if you want to overwrite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or alternatively load existing classifer model\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(clsf_train_set[:, 1:-1])\n",
    "\n",
    "bdt_classifier_savedir = \"./trained_classifiers_tree_idealized-ad_0/\"\n",
    "bdt_classifier_model = HGBClassifier(save_path=bdt_classifier_savedir,\n",
    "                                     load=True)\n",
    "bdt_full_model = make_pipeline(scaler, bdt_classifier_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's evaluate the signal extraction performance on the same test set\n",
    "\n",
    "clsf_test_set = np.vstack([innerdata_test,\n",
    "                           innerdata_extrabkg_test,\n",
    "                           innerdata_extrasig_test])\n",
    "\n",
    "X_test = clsf_test_set[:, 1:-1]\n",
    "y_test = clsf_test_set[:, -1]\n",
    "\n",
    "bdt_preds_test = bdt_full_model.predict(X_test).flatten()\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    bdt_fpr, bdt_tpr, _ = roc_curve(y_test, bdt_preds_test)\n",
    "    bdt_bkg_rej = 1 / bdt_fpr\n",
    "    bdt_sic = bdt_tpr / np.sqrt(bdt_fpr)\n",
    "\n",
    "    random_tpr = np.linspace(0, 1, 300)\n",
    "    random_bkg_rej = 1 / random_tpr\n",
    "    random_sic = random_tpr / np.sqrt(random_tpr)\n",
    "\n",
    "# SIC curve\n",
    "plt.plot(bdt_tpr, bdt_sic, label=\"idealized AD, BDT\")\n",
    "plt.plot(random_tpr, random_sic, \"w:\", label=\"random\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance in terms of SICs should be roughly the same on these input data. Of course, there is some run-by-run variance, which we could quantify more thoroughly by plotting the median and 68% CL bands, as in the end of `demos/weak_supervision.ipynb`. But for a first check we see that the BDT performs similarly to the NN, while being significantly faster to train.\n",
    "\n",
    "Now let's put the two models to the test of how they react to uninformative features. We will add ten features of pure Gaussian noise (thus abbreviating to 10G), without any discrimination power between signal and background. Thus, most input dimensions will now be useless for the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_n_noise_features(X, n=10):\n",
    "    # note that the last column is the signal-vs-background label\n",
    "    data = X[:, :-1]\n",
    "    labels = X[:, -1:]\n",
    "    noise = np.random.normal(size=(X.shape[0], n))\n",
    "    return np.hstack([data, noise, labels])\n",
    "\n",
    "\n",
    "# we consistently add the noise to training, validation and test set\n",
    "innerdata_train_noisy = add_n_noise_features(innerdata_train)\n",
    "innerdata_val_noisy = add_n_noise_features(innerdata_val)\n",
    "innerdata_test_noisy = add_n_noise_features(innerdata_test)\n",
    "\n",
    "innerdata_extrabkg_train_noisy = add_n_noise_features(innerdata_extrabkg_train)\n",
    "innerdata_extrabkg_val_noisy = add_n_noise_features(innerdata_extrabkg_val)\n",
    "innerdata_extrabkg_test_noisy = add_n_noise_features(innerdata_extrabkg_test)\n",
    "\n",
    "innerdata_extrasig_test_noisy = add_n_noise_features(innerdata_extrasig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning label 1 to \"data\"\n",
    "clsf_train_data_noisy = innerdata_train_noisy.copy()\n",
    "clsf_train_data_noisy[:, -1] = np.ones_like(clsf_train_data_noisy[:, -1])\n",
    "clsf_val_data_noisy = innerdata_val_noisy.copy()\n",
    "clsf_val_data_noisy[:, -1] = np.ones_like(clsf_val_data_noisy[:, -1])\n",
    "\n",
    "# and label 0 to background\n",
    "clsf_train_bkg_noisy = innerdata_extrabkg_train_noisy.copy()\n",
    "clsf_train_bkg_noisy[:, -1] = np.zeros_like(clsf_train_bkg_noisy[:, -1])\n",
    "clsf_val_bkg_noisy = innerdata_extrabkg_val_noisy.copy()\n",
    "clsf_val_bkg_noisy[:, -1] = np.zeros_like(clsf_val_bkg_noisy[:, -1])\n",
    "\n",
    "# mixing together and shuffling\n",
    "clsf_train_set_noisy = np.vstack([clsf_train_data_noisy, clsf_train_bkg_noisy])\n",
    "clsf_val_set_noisy = np.vstack([clsf_val_data_noisy, clsf_val_bkg_noisy])\n",
    "clsf_train_set_noisy = shuffle(clsf_train_set_noisy, random_state=42)\n",
    "clsf_val_set_noisy = shuffle(clsf_val_set_noisy, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either train new NN classifier\n",
    "\n",
    "scaler_noisy = StandardScaler()\n",
    "scaler_noisy.fit(clsf_train_set_noisy[:, 1:-1])\n",
    "\n",
    "X_train = scaler_noisy.transform(clsf_train_set_noisy[:, 1:-1])\n",
    "y_train = clsf_train_set_noisy[:, -1]\n",
    "X_val = scaler_noisy.transform(clsf_val_set_noisy[:, 1:-1])\n",
    "y_val = clsf_val_set_noisy[:, -1]\n",
    "\n",
    "nn_classifier_savedir_noisy = \"./trained_classifiers_idealized-ad_10G_0/\"\n",
    "# Let's protect ourselves from accidentally overwriting a trained model.\n",
    "if not exists(join(nn_classifier_savedir_noisy, \"CLSF_models\")):\n",
    "    nn_classifier_model_noisy = NeuralNetworkClassifier(save_path=nn_classifier_savedir_noisy,\n",
    "                                                        n_inputs=X_train.shape[1],\n",
    "                                                        early_stopping=True, epochs=None,\n",
    "                                                        verbose=True)\n",
    "    nn_classifier_model_noisy.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # merge scaler and classifier into a single pipeline\n",
    "    nn_full_model_noisy = make_pipeline(scaler_noisy, nn_classifier_model_noisy)\n",
    "else:\n",
    "    print(f\"The model exists already in {nn_classifier_savedir_noisy}. Remove first if you want to overwrite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or alternatively load existing classifer model\n",
    "\n",
    "scaler_noisy = StandardScaler()\n",
    "scaler_noisy.fit(clsf_train_set_noisy[:, 1:-1])\n",
    "\n",
    "nn_classifier_savedir_noisy = \"./trained_classifiers_idealized-ad_10G_0/\"\n",
    "nn_classifier_model_noisy = NeuralNetworkClassifier(save_path=nn_classifier_savedir_noisy,\n",
    "                                                    n_inputs=clsf_train_set_noisy[:, 1:-1].shape[1],\n",
    "                                                    load=True)\n",
    "nn_full_model_noisy = make_pipeline(scaler_noisy, nn_classifier_model_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's evaluate the signal extraction performance on the same test set\n",
    "\n",
    "clsf_test_set_noisy = np.vstack([innerdata_test_noisy,\n",
    "                           innerdata_extrabkg_test_noisy,\n",
    "                           innerdata_extrasig_test_noisy])\n",
    "\n",
    "X_test_noisy = clsf_test_set_noisy[:, 1:-1]\n",
    "y_test_noisy = clsf_test_set_noisy[:, -1]\n",
    "\n",
    "nn_preds_test_noisy = nn_full_model_noisy.predict(X_test_noisy).flatten()\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    nn_fpr_noisy, nn_tpr_noisy, _ = roc_curve(y_test_noisy, nn_preds_test_noisy)\n",
    "    nn_bkg_rej_noisy = 1 / nn_fpr_noisy\n",
    "    nn_sic_noisy = nn_tpr_noisy / np.sqrt(nn_fpr_noisy)\n",
    "\n",
    "    random_tpr = np.linspace(0, 1, 300)\n",
    "    random_bkg_rej = 1 / random_tpr\n",
    "    random_sic = random_tpr / np.sqrt(random_tpr)\n",
    "\n",
    "# SIC curve\n",
    "plt.plot(nn_tpr_noisy, nn_sic_noisy, label=\"idealized AD, NN, 10G\")\n",
    "plt.plot(random_tpr, random_sic, \"w:\", label=\"random\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network reacts poorly to this change, with only a fraction of the previous SIC left :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either train new BDT classifier\n",
    "\n",
    "scaler_noisy = StandardScaler()\n",
    "scaler_noisy.fit(clsf_train_set_noisy[:, 1:-1])\n",
    "\n",
    "X_train = scaler_noisy.transform(clsf_train_set_noisy[:, 1:-1])\n",
    "y_train = clsf_train_set_noisy[:, -1]\n",
    "X_val = scaler_noisy.transform(clsf_val_set_noisy[:, 1:-1])\n",
    "y_val = clsf_val_set_noisy[:, -1]\n",
    "\n",
    "bdt_classifier_savedir_noisy = \"./trained_classifiers_tree_idealized-ad_10G_0/\"\n",
    "# Let's protect ourselves from accidentally overwriting a trained model.\n",
    "if not exists(join(bdt_classifier_savedir_noisy, \"CLSF_models\")):\n",
    "    bdt_classifier_model_noisy = HGBClassifier(save_path=bdt_classifier_savedir_noisy,\n",
    "                                               early_stopping=True, max_iters=None,\n",
    "                                               verbose=True)\n",
    "    bdt_classifier_model_noisy.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # merge scaler and classifier into a single pipeline\n",
    "    bdt_full_model_noisy = make_pipeline(scaler_noisy, bdt_classifier_model_noisy)\n",
    "else:\n",
    "    print(f\"The model exists already in {bdt_classifier_savedir_noisy}. Remove first if you want to overwrite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or alternatively load existing classifer model\n",
    "\n",
    "scaler_noisy = StandardScaler()\n",
    "scaler_noisy.fit(clsf_train_set_noisy[:, 1:-1])\n",
    "\n",
    "bdt_classifier_savedir_noisy = \"./trained_classifiers_tree_idealized-ad_10G_0/\"\n",
    "bdt_classifier_model_noisy = HGBClassifier(save_path=bdt_classifier_savedir_noisy,\n",
    "                                           load=True)\n",
    "bdt_full_model_noisy = make_pipeline(scaler_noisy, bdt_classifier_model_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's evaluate the signal extraction performance on the same test set\n",
    "\n",
    "clsf_test_set_noisy = np.vstack([innerdata_test_noisy,\n",
    "                                 innerdata_extrabkg_test_noisy,\n",
    "                                 innerdata_extrasig_test_noisy])\n",
    "\n",
    "X_test_noisy = clsf_test_set_noisy[:, 1:-1]\n",
    "y_test_noisy = clsf_test_set_noisy[:, -1]\n",
    "\n",
    "bdt_preds_test_noisy = bdt_full_model_noisy.predict(X_test_noisy).flatten()\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    bdt_fpr_noisy, bdt_tpr_noisy, _ = roc_curve(y_test_noisy, bdt_preds_test_noisy)\n",
    "    bdt_bkg_rej_noisy = 1 / bdt_fpr_noisy\n",
    "    bdt_sic_noisy = bdt_tpr_noisy / np.sqrt(bdt_fpr_noisy)\n",
    "\n",
    "    random_tpr = np.linspace(0, 1, 300)\n",
    "    random_bkg_rej = 1 / random_tpr\n",
    "    random_sic = random_tpr / np.sqrt(random_tpr)\n",
    "\n",
    "# SIC curve\n",
    "plt.plot(bdt_tpr_noisy, bdt_sic_noisy, label=\"idealized AD, BDT, 10G\")\n",
    "plt.plot(random_tpr, random_sic, \"w:\", label=\"random\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is likely also a drop in performance in the BDT, compared to before, but the remaining SIC is substantially higher than the NN case. Plus, it was much faster to train.\n",
    "\n",
    "But to get more out of the BDT, we should train an ensemble of multiple BDTs, each with a different separation into training and validation data. Since BDTs train so fast, this is not really a problem. We use an ensemble of ten models here for illustration, but we could do even better with larger ensembles.\n",
    "\n",
    "We conveniently implement the model here with a wrapper class `EnsembleModel`, which has the same API as a single model and takes care of the averaging of provided models under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix together train and validation set for different splits\n",
    "innerdata_train_val_noisy = np.vstack([innerdata_train_noisy,\n",
    "                                       innerdata_val_noisy])\n",
    "innerdata_extrabkg_train_val_noisy = np.vstack([innerdata_extrabkg_train_noisy,\n",
    "                                                innerdata_extrabkg_val_noisy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train ensemble of trees with different train/val splits\n",
    "n_classifiers = 10\n",
    "\n",
    "model_noisy_list = []\n",
    "for i in range(n_classifiers):\n",
    "\n",
    "    # different split per classifier\n",
    "    # (could also do this more controlled via a fixed k-folding scheme)\n",
    "    _innerdata_train, _innerdata_val = train_test_split(\n",
    "        innerdata_train_val_noisy, train_size=0.8, random_state=1337+i)\n",
    "    _innerdata_extrabkg_train, _innerdata_extrabkg_val = train_test_split(\n",
    "        innerdata_extrabkg_train_val_noisy, train_size=0.8, random_state=1337+i)\n",
    "\n",
    "    # assigning label 1 to \"data\"\n",
    "    _clsf_train_data = _innerdata_train\n",
    "    _clsf_train_data[:, -1] = np.ones_like(_clsf_train_data[:, -1])\n",
    "    _clsf_val_data = _innerdata_val\n",
    "    _clsf_val_data[:, -1] = np.ones_like(_clsf_val_data[:, -1])\n",
    "\n",
    "    # and label 0 to background\n",
    "    _clsf_train_bkg = _innerdata_extrabkg_train\n",
    "    _clsf_train_bkg[:, -1] = np.zeros_like(_clsf_train_bkg[:, -1])\n",
    "    _clsf_val_bkg = _innerdata_extrabkg_val\n",
    "    _clsf_val_bkg[:, -1] = np.zeros_like(_clsf_val_bkg[:, -1])\n",
    "\n",
    "    # mixing together and shuffling\n",
    "    _clsf_train_set = np.vstack([_clsf_train_data, _clsf_train_bkg])\n",
    "    _clsf_val_set = np.vstack([_clsf_val_data, _clsf_val_bkg])\n",
    "    _clsf_train_set = shuffle(_clsf_train_set, random_state=42)\n",
    "    _clsf_val_set = shuffle(_clsf_val_set, random_state=42)\n",
    "\n",
    "    # fit scaler\n",
    "    _scaler = StandardScaler()\n",
    "    _scaler.fit(_clsf_train_set[:, 1:-1])\n",
    "\n",
    "    # train classifier\n",
    "    _classifier_savedir = f\"./trained_classifier_idealized-ad_ensemble_10G/model_{i}/\"\n",
    "    _classifier = HGBClassifier(save_path=_classifier_savedir,\n",
    "                                early_stopping=True, max_iters=None,\n",
    "                                verbose=False)\n",
    "\n",
    "    # We don't want to overwrite the model if it already exists.\n",
    "    if not exists(join(_classifier_savedir, \"CLSF_models\")):\n",
    "        X_train = _scaler.transform(_clsf_train_set[:, 1:-1])\n",
    "        y_train = _clsf_train_set[:, -1]\n",
    "        X_val = _scaler.transform(_clsf_val_set[:, 1:-1])\n",
    "        y_val = _clsf_val_set[:, -1]\n",
    "        _classifier.fit(X_train, y_train, X_val, y_val)\n",
    "    else:\n",
    "        print(f\"The model exists already in {_classifier_savedir}. Remove first if you want to overwrite. Loading its best state now.\")\n",
    "        _classifier.load_best_model()\n",
    "\n",
    "    # merge scaler and classifier into a single pipeline model\n",
    "    _pipeline = make_pipeline(_scaler, _classifier)\n",
    "    model_noisy_list.append(_pipeline)\n",
    "\n",
    "# Now merging all these models into a single ensemble model\n",
    "ensemble_noisy = EnsembleModel(model_noisy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's evaluate the signal extraction performance on the same test set\n",
    "\n",
    "clsf_test_set_noisy = np.vstack([innerdata_test_noisy,\n",
    "                                 innerdata_extrabkg_test_noisy,\n",
    "                                 innerdata_extrasig_test_noisy])\n",
    "\n",
    "X_test_noisy = clsf_test_set_noisy[:, 1:-1]\n",
    "y_test_noisy = clsf_test_set_noisy[:, -1]\n",
    "\n",
    "ensemble_preds_test_noisy = ensemble_noisy.predict(X_test_noisy).flatten()\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    ensemble_fpr_noisy, ensemble_tpr_noisy, _ = roc_curve(y_test_noisy, ensemble_preds_test_noisy)\n",
    "    ensemble_bkg_rej_noisy = 1 / ensemble_fpr_noisy\n",
    "    ensemble_sic_noisy = ensemble_tpr_noisy / np.sqrt(ensemble_fpr_noisy)\n",
    "\n",
    "    random_tpr = np.linspace(0, 1, 300)\n",
    "    random_bkg_rej = 1 / random_tpr\n",
    "    random_sic = random_tpr / np.sqrt(random_tpr)\n",
    "\n",
    "# SIC curve\n",
    "plt.plot(ensemble_tpr_noisy, ensemble_sic_noisy, label=\"idealized AD, BDT ensemble, 10G\")\n",
    "plt.plot(random_tpr, random_sic, \"w:\", label=\"random\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see another substantial improvement compared to the single tree model. In fact, the performance of an ensemble model tends to be higher than the performance of each individual model that go into the ensemble, i.e. it's not just trivially averaging out the run-by-run variance. A way to understand this is that a single event might get a high (close to 1) prediction by one model but background-like (close to 0.5) prediction by the others. The average of these predictions is still shifted upwards compared to the case where all models agree on a background-like prediction. NNs can also benefit from this type of ensembling, but in this case it will not be sufficient to recover the performance drop due to the noise, plus their longer training times makes ensembling less cheap.\n",
    "\n",
    "In summary, we have now seen that BDTs are a nice alternative to neural networks in weakly supervised anomaly detection for two reasons:\n",
    "\n",
    "1) they are more resilient to uninformative input features\n",
    "2) they are much faster to train\n",
    "\n",
    "The latter point makes it very attractive to squeeze out more performance by ensembling over many individually trained BDTs (which are technically also making use of ensembling under the hood individually)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NotebookEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
