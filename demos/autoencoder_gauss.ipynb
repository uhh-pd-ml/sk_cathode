{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463e3721-ef49-4fc6-b9aa-de08ba3372a5",
   "metadata": {},
   "source": [
    "# Outlier Detection (Gaussian Toy Example)\n",
    "\n",
    "In this notebook, we will demonstrate the basics of outlier detection in the context of anomaly detection. \n",
    "We will use simple Gaussian toy data to demonstrate the basic concepts. \n",
    "\n",
    "In the outlier detection version of anomaly detection, we train a model to learn what our background looks like\n",
    "and then classify things as anomalous based on how 'disimilar' they look as compared to the background. \n",
    "\n",
    "Essentially this means we are defining events that have low probability density under the background to be anomalous. \n",
    "In this case, we are generating our own Gaussian toy data, so we know the true probability distribution of the background.\n",
    "However, in realistic physics examples this is usually not the case. \n",
    "One must therefore train a machine learning model to learn the background probability distribution, or an equivalent proxy, from a sample of background events.\n",
    "\n",
    "One common proxy used to learn the background distribution is a type of neural network called an autoencoder.\n",
    "Autoencoders do not directly learn the probability distribution. Instead they are trained to take the input data, compress it down into some smaller representation\n",
    "and decompress it back out to recover the original inputs. The idea is that by forcing the model to learn to compress the data, it will force it to learn its underlying structure.\n",
    "If the model is trained only on background events, it should hopefully learn how to do this compression task for background events but not for signal events.\n",
    "Therefore, there should be a larger difference between the model input and output on signal events. \n",
    "This difference, called the reconstruction loss, can therefore be used as an anomaly score.\n",
    "\n",
    "Note that unlike weak supervision, we expect this type model to always be worse than a supervised classifier because it never sees signal events during the training.\n",
    "However, it can usuaully be trained in an easier fashion, (because one only needs to find a sample of background events) and has a stable performance instead of varying depending on the amount of signal present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec5494d-cb5d-454c-8f7e-06116ad5b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sys\n",
    "\n",
    "from os.path import exists, join, dirname, realpath\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# adding parent directory to path\n",
    "parent_dir = dirname(realpath(globals()[\"_dh\"][0]))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from sk_cathode.generative_models.autoencoder import Autoencoder\n",
    "from sk_cathode.classifier_models.neural_network_classifier import NeuralNetworkClassifier\n",
    "from sk_cathode.utils.evaluation_functions import plot_roc_and_sic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c4aa9-ff1a-4ed8-9744-1a2506689abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# :sunglasses:\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a45aec6-6504-43fb-b353-f9cabac3d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the dimensionality of our dataset\n",
    "n_dim = 10  # How many total dimensions of our data\n",
    "n_signal_dim = 2  # How many dimensions of our signal are different from background\n",
    "\n",
    "# Background is multi-dim Gaussian with zero mean, diagonal covariance of one\n",
    "bkg_means = np.array([0.]*n_dim)\n",
    "bkg_vars = np.ones(n_dim)\n",
    "bkg_cov = np.diag(bkg_vars)\n",
    "bkg_pdf = scipy.stats.multivariate_normal(bkg_means, bkg_cov)\n",
    "\n",
    "# Signal is multi-dim Gaussian centered at 1 for 'signal like dimensions and 0 for the bkg-like dimensions\n",
    "sig_means = np.array([2.5] * n_signal_dim + [0.] * (n_dim - n_signal_dim))\n",
    "sig_vars = np.array(n_signal_dim * [0.1] + [1.0]* (n_dim - n_signal_dim))\n",
    "sig_cov = np.diag(sig_vars)\n",
    "sig_pdf = scipy.stats.multivariate_normal(sig_means, sig_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062e063f-dd1e-4bc0-b7e7-f50baeed935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for training of autoencoder classifer\n",
    "n_bkg = 100000\n",
    "bkg_events_train = bkg_pdf.rvs(size=n_bkg)\n",
    "\n",
    "# Data for training of supervised classifer\n",
    "n_sup = 10000\n",
    "sig_events_sup = sig_pdf.rvs(size=n_sup)\n",
    "bkg_events_sup = bkg_pdf.rvs(size=n_sup)\n",
    "\n",
    "x_sup = np.append(sig_events_sup, bkg_events_sup, axis=0)\n",
    "y_sup = np.append(np.ones(n_sup, dtype=np.int8), np.zeros(n_sup, dtype=np.int8))\n",
    "\n",
    "x_sup, y_sup = shuffle(x_sup, y_sup, random_state=42)\n",
    "x_sup_train, x_sup_val, y_sup_train, y_sup_val = train_test_split(x_sup, y_sup, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data for testing\n",
    "n_test = 50000\n",
    "sig_events_test = sig_pdf.rvs(size=n_test//10)\n",
    "bkg_events_test = bkg_pdf.rvs(size=n_test)\n",
    "\n",
    "x_test = np.append(sig_events_test, bkg_events_test, axis=0)\n",
    "y_test = np.append(np.ones(n_test//10, dtype=np.int8), np.zeros(n_test, dtype=np.int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd4781-ff73-4226-bdd4-d9b63cd7ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple scatter plot of the first two dimensions of our data, background is in blue, signal is in orange\n",
    "plt.figure(figsize = (5, 5))\n",
    "plt.scatter(bkg_events_test[:, 0], bkg_events_test[:, 1], s=0.3, color='C0')\n",
    "plt.scatter(sig_events_test[:, 0], sig_events_test[:, 1], s=0.3, color='C1')\n",
    "plt.gca().set_aspect(1.)\n",
    "plt.xlabel(r'$x_1$', fontsize=16)\n",
    "plt.ylabel(r'$x_2$', fontsize=16)\n",
    "plt.xlim([-4, 4])\n",
    "plt.ylim([-4, 4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0a19e3-556d-4439-ba4f-f1024d1c46b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE model\n",
    "\n",
    "# Pick size of compressed representation (latent)\n",
    "latent_size = 4\n",
    "layers = [n_dim, 32, 16, latent_size, 16, 32, n_dim]\n",
    "\n",
    "ae_model = Autoencoder(n_inputs=n_dim, \n",
    "                       layers=layers, \n",
    "                       val_split=0.1,\n",
    "                       early_stopping=True, \n",
    "                       epochs=100, verbose=True)\n",
    "ae_model.fit(bkg_events_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65265e9c-79e6-4243-b8ea-79b0a58d6846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a quick check of the performance of the autoencoder as a classifier \n",
    "\n",
    "# predict_proba method computes MSE loss which we use as 'anomaly score' for each event \n",
    "y_test_ae = ae_model.predict_proba(x_test)\n",
    "auc_ae = roc_auc_score(y_test, y_test_ae)\n",
    "\n",
    "print(f\"AE AUC {auc_ae:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae4fe23-07ab-435a-9fbd-6f01894d7e86",
   "metadata": {},
   "source": [
    "In order to gauge how well our autoencoder is doing we can compare it to serval benchmarks.\n",
    "\n",
    "First we can see how well evaluating the true background pdf would do. We expect this to to similar to a well performing autoencder\n",
    "We also train a supervised classifier and compute the likelihood ratio. We expect these to be sgnificantly better than the outlier detection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62682193-600f-4bd4-a937-2d4eeec532d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a quick check of the performance of the true bkg pdf as a classifier \n",
    "x_test_bkg_pdf = bkg_pdf.pdf(x_test)\n",
    "\n",
    "# One over bkg probability as outlier score \n",
    "y_test_pdf = 1./x_test_bkg_pdf\n",
    "\n",
    "auc_pdf = roc_auc_score(y_test, y_test_pdf)\n",
    "print(f\"bkg PDF AUC {auc_pdf:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928ac64d-63fb-42e0-b737-94a6acac00e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use exact likelihood ratio to get optimal performance likelihood ratio\n",
    "x_test_sig_pdf = sig_pdf.pdf(x_test)\n",
    "likelihood_ratio = x_test_sig_pdf / x_test_bkg_pdf\n",
    "\n",
    "auc_ratio = roc_auc_score(y_test, likelihood_ratio)\n",
    "print(f\"likelihood ratio AUC {auc_ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351f295f-f9db-4687-8c79-65e5e91ce9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a supervised model for comparison\n",
    "\n",
    "sup_model = NeuralNetworkClassifier(n_inputs=n_dim,\n",
    "                                    early_stopping=True, epochs=50,\n",
    "                                    verbose=True)\n",
    "sup_model.fit(x_sup_train, y_sup_train, x_sup_val, y_sup_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00716bb4-914d-437f-b75a-022f28fcd381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a quick check of the performance of the supervised classifier \n",
    "y_test_sup = sup_model.predict(x_test)\n",
    "auc_sup = roc_auc_score(y_test, y_test_sup)\n",
    "\n",
    "print(f\"Supervised AUC {auc_sup:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc5a24-11f3-43d4-b029-7cd9eafb2144",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_and_sic(y_test,\n",
    "                 [likelihood_ratio, y_test_sup, y_test_ae, y_test_pdf],\n",
    "                 labels = ['Likelihood Ratio', 'Supervised', 'Autoencoder',  'True Background PDF'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac720922-42f8-4067-b397-54dc9b3ab78d",
   "metadata": {},
   "source": [
    "We can see that the outlier detection methods, either based on the true background PDF or the autoencoder fall below the sensitivity of a supervised classifier. \n",
    "However they both are able to successfully enhance the sensitivity to the signal, by factors greater than ~2. \n",
    "The autoencoder performance is decently close to the true background PDF, which is encouraging. \n",
    "\n",
    "Feel free to now play around with how the results change if you change the latent size of the autoencoder, or change the signal or background PDF's.\n",
    "Perhaps you can also compare the autoencoder performance to that of weak supervision on the same dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
